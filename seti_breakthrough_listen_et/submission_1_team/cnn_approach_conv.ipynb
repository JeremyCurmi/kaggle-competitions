{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../source_code')\n",
    "from source_code.utils import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/jeremy/data/'\n",
    "file_name = 'seti_breakthrough_listen_et'\n",
    "data_path = data_path + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TARGET = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_file_path(image_id):\n",
    "    return f\"{data_path}/train/{image_id[0]}/{image_id}.npy\"\n",
    "\n",
    "def get_test_file_path(image_id):\n",
    "    return f\"{data_path}/test/{image_id[0]}/{image_id}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path + '/train_labels.csv')\n",
    "train['file_path'] = train['id'].apply(get_train_file_path)\n",
    "train['file_path'] = train['file_path']#.str.split(prefix).str[-1]\n",
    "\n",
    "test = pd.read_csv(data_path + '/sample_submission.csv')\n",
    "test['file_path'] = test['id'].apply(get_test_file_path)\n",
    "test['file_path'] = test['file_path']#.str.split(prefix).str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45148 5017 35847\n"
     ]
    }
   ],
   "source": [
    "train_size = int(train_df.shape[0])\n",
    "validation_size = int(validation_df.shape[0])\n",
    "test_size = int(test.shape[0])\n",
    "print(train_size, validation_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 45148/45148 [01:58<00:00, 380.15it/s]\n",
      "100%|██████████████████████████████████████| 5017/5017 [00:13<00:00, 381.55it/s]\n",
      " 84%|██████████████████████████████      | 29933/35847 [02:55<00:15, 373.21it/s]"
     ]
    }
   ],
   "source": [
    "train_examples = []\n",
    "validation_examples = []\n",
    "test_examples = []\n",
    "\n",
    "train_labels = []\n",
    "validation_labels = []\n",
    "test_indexes = []\n",
    "\n",
    "for i in tqdm(train_df.index[:train_size].to_list()):\n",
    "    train_examples.append(np.load(train_df.loc[i,'file_path']).reshape(6,273,256))\n",
    "    train_labels.append(train_df.loc[i,TARGET])\n",
    "\n",
    "for i in tqdm(validation_df.index[:validation_size].to_list()):\n",
    "    validation_examples.append(np.load(validation_df.loc[i,'file_path']).reshape(6,273,256))\n",
    "    validation_labels.append(validation_df.loc[i,TARGET])\n",
    "\n",
    "for i in tqdm(test.index[:test_size].to_list()):\n",
    "    test_examples.append(np.load(test.loc[i,'file_path']).reshape(6,273,256))\n",
    "    test_indexes.append(test.loc[i, 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_examples, validation_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "EPOCHS = 30\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(train['target'].values),\n",
    "                                                  train['target'].values)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights[1] = class_weights[1]*0.5\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(6, 273, 256)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True,)\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data = validation_dataset, callbacks=[callbacks], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "current_timestamp_ = current_timestamp()\n",
    "save_model(model, f'cnn_model_{current_timestamp_}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses[['auc','val_auc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(validation_dataset).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_preds = (preds > 0.5).astype(int)\n",
    "confusion_matrix(validation_labels, binary_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(validation_labels, binary_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(validation_examples[0].shape, test_examples[0].shape)\n",
    "print(len(validation_examples), len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission_pred = model.predict(test_dataset).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame([test_indexes, submission_pred], index=['id','target']).T\n",
    "submission.to_csv(f'submission_{current_timestamp_}.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
